import json
import logging
import pyarrow as pa
import pyarrow.json as pj
import pyarrow.csv as pv
from extractors.base_extractor import BaseExtractor

class FileDataExtractor(BaseExtractor):
    NAME = 'FILE_DATA_EXTRACTOR'

    def __init__(self , app_name: str , config: dict , transformer) -> None:
        super().__init__(app_name, transformer, config)
        self._file_directory = config.get('directory' , '')
        self._file_name_include = config.get('filters')['include']
        self._get_last_file = config.get('get_last')
        self._file_type = config.get('filters')['suffix']

    def _fetch(self):
        logging.info('Reading data from file...')

        file_pattern = f'{self._file_name_include}_*{self._file_type}'

        file_list = f'{self._file_directory}{file_pattern}'

        # file_path = f'{self._file_directory}{self._file_name}.{self._file_type}'
        arrow_table = None

        try:
            if self._file_type == 'json':
                arrow_table = pj.read_json(file_list)
            elif self._file_type == '.csv':

                arrow_table = [pv.read_csv(file) for file in file_list]
            else:
                raise ValueError(f'Unsupported file type: {self._file_type}')


            logging.info(f'Successfully read data from files')
            return arrow_table, []
        except Exception as e:
            logging.error(f'Failed to read data from files. Error: {str(e)}')
            return None, [str(e)]

-------

appName: FileDataToFile

meta:
  logging_level: INFO

extract:
  name: FILE_DATA_EXTRACTOR
  directory: etl/output/
  # housekeeping:
  # reprocess:
  filters:
    get_last: true
    # days_back:
    include:
      - "weather_data"
    suffix:
      - .csv

transformer:
  name: TRANSFORMER_CHAIN
  transformers:
    - name: NO_OPP_TRANSFORMER

storage:
  name: FILE_STORAGE
  directory: output/
  file_name: all_weather_data
  file_type: .csv
  

